{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pykonal\n",
    "import seispy\n",
    "\n",
    "DTYPE_REAL = np.float64\n",
    "EARTH_RADIUS = 6371.\n",
    "\n",
    "def geo2sph(arr) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Map Geographical coordinates to spherical coordinates.\n",
    "    \"\"\"\n",
    "    geo = np.array(arr, dtype=DTYPE_REAL)\n",
    "    sph = np.empty_like(geo)\n",
    "    sph[..., 0] = EARTH_RADIUS - geo[..., 2]\n",
    "    sph[..., 1] = np.pi / 2 - np.radians(geo[..., 0])\n",
    "    sph[..., 2] = np.radians(geo[..., 1])\n",
    "    return (sph)\n",
    "\n",
    "\n",
    "def sph2geo(arr) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Map spherical coordinates to geographic coordinates.\n",
    "    \"\"\"\n",
    "    sph = np.array(arr, dtype=DTYPE_REAL)\n",
    "    geo = np.empty_like(sph)\n",
    "    geo[..., 0] = np.degrees(np.pi / 2 - sph[..., 1])\n",
    "    geo[..., 1] = np.degrees(sph[..., 2])\n",
    "    geo[..., 2] = EARTH_RADIUS - sph[..., 0]\n",
    "    return (geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load velocity model\n",
    "grid = pykonal.fields.ScalarField3D(coord_sys=\"spherical\")\n",
    "with np.load(\"/home/malcolmw/google_drive/malcolm.white@usc.edu/data/velocity/White_et_al_2019a/White_et_al_2019a.regular.npz\") as npz:\n",
    "        grid.min_coords = npz[\"grid_parameters\"][:3]\n",
    "        grid.node_intervals = npz[\"grid_parameters\"][3:6]\n",
    "        grid.npts = npz[\"grid_parameters\"][6:] + [1, 0, 0]\n",
    "        pwave_velocity = np.append(npz[\"vp\"], npz[\"vp\"][[-1]], axis=0)\n",
    "        swave_velocity = np.append(npz[\"vs\"], npz[\"vs\"][[-1]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "db = seispy.pandas.catalog.Catalog(\n",
    "    \"/home/malcolmw/google_drive/malcolm.white@usc.edu/data/events/malcolmw/SJFZ_catalog_2008-2016.h5\",\n",
    "    fmt=\"hdf5\"\n",
    ")\n",
    "\n",
    "stations_dataframe = db[\"site\"][\n",
    "    [\"sta\", \"lat\", \"lon\", \"elev\"]\n",
    "].drop_duplicates(\n",
    "    \"sta\"\n",
    ").merge(\n",
    "    db[\"snetsta\"][\n",
    "        [\"snet\", \"sta\"]\n",
    "    ].drop_duplicates(\n",
    "        \"sta\"\n",
    "    ),\n",
    "    on=\"sta\"\n",
    ")\n",
    "stations_dataframe[\"station_id\"] = stations_dataframe[\"snet\"] + \".\" + stations_dataframe[\"sta\"]\n",
    "\n",
    "arrivals_dataframe = db[\"arrival\"][\n",
    "    [\"sta\", \"time\", \"arid\"]\n",
    "].merge(\n",
    "    db[\"assoc\"][\n",
    "        [\"arid\", \"orid\", \"phase\"]\n",
    "    ],\n",
    "    on=\"arid\"\n",
    ").merge(\n",
    "    stations_dataframe[[\"sta\", \"station_id\"]],\n",
    "    on=\"sta\"\n",
    ").merge(\n",
    "    db[\"origin\"][[\"orid\", \"evid\"]],\n",
    "    on=\"orid\"\n",
    ").sort_values(\n",
    "    \"evid\"\n",
    ").set_index(\n",
    "    \"evid\"\n",
    ")[\n",
    "    [\"station_id\", \"phase\", \"time\", \"arid\"]\n",
    "]\n",
    "\n",
    "stations_dataframe[\"depth\"] = -stations_dataframe[\"elev\"]\n",
    "stations_dataframe = stations_dataframe[\n",
    "    [\"station_id\", \"lat\", \"lon\", \"depth\"]\n",
    "].set_index(\n",
    "    \"station_id\"\n",
    ")\n",
    "stations_dataframe = stations_dataframe[\n",
    "    stations_dataframe.index.isin(arrivals_dataframe[\"station_id\"].unique())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stations_dict = dict(zip(stations_dataframe.index.values, geo2sph(stations_dataframe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Only events inside the focus region of White et al. 2019 with 128 or more arrivals\n",
    "df = arrivals_dataframe.reset_index().groupby(\"evid\").count()\n",
    "arrivals_dataframe = arrivals_dataframe[\n",
    "     (arrivals_dataframe.index.isin(df[df[\"station_id\"] >= 128].index))\n",
    "    &(arrivals_dataframe.index.isin(\n",
    "        db[\"origin\"][seispy.coords.as_geographic(db[\"origin\"][[\"lat\", \"lon\", \"depth\"]]).in_rectangle()][\"evid\"]\n",
    "        )\n",
    "     )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_arrivals(arrivals_dataframe):\n",
    "    for event_id in arrivals_dataframe.index.unique()[:]:\n",
    "        yield (event_id, arrivals_dataframe.loc[event_id].set_index([\"station_id\", \"phase\"]).to_dict()[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = pykonal.locate.EQLocator(stations_dict, tt_dir=\"/home/malcolmw/scratch/traveltimes\")\n",
    "locator.grid.min_coords = grid.min_coords\n",
    "locator.grid.node_intervals = grid.node_intervals\n",
    "locator.grid.npts = grid.npts\n",
    "locator.pwave_velocity = pwave_velocity\n",
    "locator.swave_velocity = swave_velocity\n",
    "#     locator.compute_all_traveltime_lookup_tables(\"P\")\n",
    "#     locator.compute_all_traveltime_lookup_tables(\"S\")\n",
    "for event_id, arrivals in generate_arrivals(arrivals_dataframe):\n",
    "    locator.clear_arrivals()\n",
    "    locator.add_arrivals(arrivals)\n",
    "    locator.load_traveltimes()\n",
    "    %time loc0 = locator.grid_search()\n",
    "    %time loc1 = locator.locate()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [locator.arrivals[key] for key in sorted(locator.arrivals)], \n",
    "[locator.traveltimes[key] for key in sorted(locator.arrivals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator.grid.max_coords, locator.grid.min_coords + locator.grid.node_intervals * (locator.grid.npts - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sph2geo(loc0[:3]), sph2geo(loc1[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator.cost(loc0), locator.cost(loc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "cimport numpy as np\n",
    "\n",
    "\n",
    "ctypedef np.float64_t _REAL_t\n",
    "\n",
    "EARTH_RADIUS = 6371.\n",
    "DTYPE_REAL = np.float64\n",
    "\n",
    "def geo2sph(arr):\n",
    "    \"\"\"\n",
    "    Map Geographical coordinates to spherical coordinates.\n",
    "    \"\"\"\n",
    "    geo = np.array(arr, dtype=DTYPE_REAL)\n",
    "    sph = np.empty_like(geo)\n",
    "    sph[..., 0] = EARTH_RADIUS - geo[..., 2]\n",
    "    sph[..., 1] = np.pi / 2 - np.radians(geo[..., 0])\n",
    "    sph[..., 2] = np.radians(geo[..., 1])\n",
    "    return (sph)\n",
    "\n",
    "\n",
    "def sph2geo(arr):\n",
    "    \"\"\"\n",
    "    Map spherical coordinates to geographic coordinates.\n",
    "    \"\"\"\n",
    "    sph = np.array(arr, dtype=DTYPE_REAL)\n",
    "    geo = np.empty_like(sph)\n",
    "    geo[..., 0] = np.degrees(np.pi / 2 - sph[..., 1])\n",
    "    geo[..., 1] = np.degrees(sph[..., 2])\n",
    "    geo[..., 2] = EARTH_RADIUS - sph[..., 0]\n",
    "    return (geo)\n",
    "\n",
    "\n",
    "cdef class EQLocator(object):\n",
    "    cdef dict _arrivals\n",
    "    cdef dict _tt_calculators\n",
    "    cdef _REAL_t[:,:,:,:] _grid\n",
    "    cdef tuple _bounds\n",
    "    cdef dict _priors\n",
    "    \n",
    "    def __init__(self, arrivals, tt_calculators, grid):\n",
    "        self.arrivals = {key: arrivals[key] for key in tt_calculators}\n",
    "        self.tt_calculators = tt_calculators\n",
    "        self.grid = grid\n",
    "    \n",
    "    @property\n",
    "    def arrivals(self):\n",
    "        return (self._arrivals)\n",
    "    \n",
    "    @arrivals.setter\n",
    "    def arrivals(self, value):\n",
    "        self._arrivals = value\n",
    "    \n",
    "    @property\n",
    "    def grid(self):\n",
    "        return (np.asarray(self._grid))\n",
    "    \n",
    "    @grid.setter\n",
    "    def grid(self, value):\n",
    "        self._grid = value\n",
    "        \n",
    "    @property\n",
    "    def tt_calculators(self):\n",
    "        return (self._tt_calculators)\n",
    "    \n",
    "    @tt_calculators.setter\n",
    "    def tt_calculators(self, value):\n",
    "        self._tt_calculators = value\n",
    "        \n",
    "    cpdef initial_guess(self):\n",
    "        values = [self.arrivals[key]-self.tt_calculators[key].values for key in self.tt_calculators]\n",
    "        values = np.ma.masked_invalid(np.stack(values))\n",
    "        std = values.std(axis=0)\n",
    "        arg_min = np.argmin(std)\n",
    "        idx_min = np.unravel_index(arg_min, std.shape)\n",
    "        geo = sph2geo(self.grid[idx_min])\n",
    "        time = values.mean(axis=0)[idx_min]\n",
    "        return (np.array([*geo, time]))\n",
    "\n",
    "    cpdef cost(self, _REAL_t[:] hypocenter):\n",
    "        cdef tuple key\n",
    "        cdef _REAL_t csum = 0\n",
    "        cdef _REAL_t lat, lon, depth, time\n",
    "        lat = hypocenter[0]\n",
    "        lon = hypocenter[1]\n",
    "        depth = hypocenter[2]\n",
    "        time = hypocenter[3]\n",
    "        sph_coords = geo2sph((lat, lon, depth))\n",
    "        if not np.all(sph_coords > np.asarray(self.grid).min(axis=(0,1,2))) or not np.all(sph_coords < np.asarray(self.grid).max(axis=(0,1,2))):\n",
    "            return (np.inf)\n",
    "        for key in self.arrivals:\n",
    "            tt_calculator = self.tt_calculators[key].value\n",
    "            csum += np.square(self.arrivals[key]-(time+tt_calculator(sph_coords)))\n",
    "        return (np.sqrt(csum/len(self.arrivals)))\n",
    "    \n",
    "    cpdef locate(self):\n",
    "        cdef _REAL_t[4] h0\n",
    "        h0 = self.initial_guess()\n",
    "        soln = scipy.optimize.differential_evolution(\n",
    "            self.cost,\n",
    "            ((h0[0]-0.1, h0[0]+0.1), (h0[1]-0.1, h0[1]+0.1), (0, 30), (h0[3]-5, h0[3]+5))\n",
    "        )\n",
    "        return (soln.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = seispy.pandas.catalog.Catalog(\n",
    "    \"/home/malcolmw/google_drive/malcolm.white@usc.edu/data/events/malcolmw/SJFZ_catalog_2008-2016.h5\",\n",
    "    fmt=\"hdf5\"\n",
    ")\n",
    "\n",
    "gc_events_dataframe = pd.read_csv(\n",
    "    \"/home/malcolmw/scratch/events.grow\", \n",
    "    delim_whitespace=True, \n",
    "    header=None,\n",
    "    names=[\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\", \"lat\", \"lon\", \"depth\", \"null_0\", \"null_1\", \"null_2\", \"null_3\", \"event_id\"]\n",
    ")\n",
    "gc_events_dataframe[\"time\"] = seispy.pandas.time.to_epoch(seispy.pandas.time.ymd_to_dt(gc_events_dataframe))*1e-9\n",
    "gc_events_dataframe[\"event_id\"] -= int(2e9)\n",
    "db[\"origin\"] = db[\"origin\"][[\"orid\", \"evid\"]].merge(\n",
    "    gc_events_dataframe[[\"lat\", \"lon\", \"depth\", \"time\", \"event_id\"]],\n",
    "    left_on=\"evid\",\n",
    "    right_on=\"event_id\"\n",
    ").drop(columns=[\"evid\"])\n",
    "\n",
    "stations_dataframe = db[\"site\"][\n",
    "    [\"sta\", \"lat\", \"lon\", \"elev\"]\n",
    "].drop_duplicates(\n",
    "    \"sta\"\n",
    ").merge(\n",
    "    db[\"snetsta\"][\n",
    "        [\"snet\", \"sta\"]\n",
    "    ].drop_duplicates(\n",
    "        \"sta\"\n",
    "    ),\n",
    "    on=\"sta\"\n",
    ")\n",
    "stations_dataframe[\"station_id\"] = stations_dataframe[\"snet\"] + \".\" + stations_dataframe[\"sta\"]\n",
    "\n",
    "arrivals_dataframe = db[\"arrival\"][\n",
    "    [\"sta\", \"time\", \"arid\"]\n",
    "].merge(\n",
    "    db[\"assoc\"][\n",
    "        [\"arid\", \"orid\", \"phase\"]\n",
    "    ],\n",
    "    on=\"arid\"\n",
    ").merge(\n",
    "    stations_dataframe[[\"sta\", \"station_id\"]],\n",
    "    on=\"sta\"\n",
    ").merge(\n",
    "    db[\"origin\"][[\"orid\", \"event_id\"]],\n",
    "    on=\"orid\"\n",
    ").sort_values(\n",
    "    \"event_id\"\n",
    ").set_index(\n",
    "    \"event_id\"\n",
    ")[\n",
    "    [\"station_id\", \"phase\", \"time\", \"arid\"]\n",
    "]\n",
    "\n",
    "stations_dataframe[\"depth\"] = -stations_dataframe[\"elev\"]\n",
    "stations_dataframe = stations_dataframe[\n",
    "    [\"station_id\", \"lat\", \"lon\", \"depth\"]\n",
    "].set_index(\n",
    "    \"station_id\"\n",
    ")\n",
    "stations_dataframe = stations_dataframe[\n",
    "    stations_dataframe.index.isin(arrivals_dataframe[\"station_id\"].unique())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and resample the velocity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"/home/malcolmw/google_drive/malcolm.white@usc.edu/data/velocity/White_et_al_2019a/White_et_al_2019a.regular.npz\") as npz:\n",
    "    vp = pykonal.field.ScalarField3D(coord_sys=\"spherical\")\n",
    "    vs = pykonal.field.ScalarField3D(coord_sys=\"spherical\")\n",
    "    vp.min_coords = vs.min_coords = npz[\"grid_parameters\"][:3]\n",
    "    vp.node_intervals = vs.node_intervals = npz[\"grid_parameters\"][3:6]\n",
    "    vp.npts = vs.npts = npz[\"grid_parameters\"][6:] + [1, 0, 0]\n",
    "    vp.values = np.append(npz[\"vp\"], npz[\"vp\"][[-1]], axis=0)\n",
    "    vs.values = np.append(npz[\"vs\"], npz[\"vs\"][[-1]], axis=0)\n",
    "    \n",
    "# Resample the velocity model (18, 32, 32) --> (64, 128, 128)\n",
    "rho_min, theta_min, phi_min = vp.min_coords\n",
    "rho_max, theta_max, phi_max = vp.max_coords\n",
    "nrho, ntheta, nphi = 64, 128, 128\n",
    "\n",
    "drho = (rho_max - rho_min) / (nrho - 1)\n",
    "rho = np.linspace(rho_min, rho_max, nrho)\n",
    "\n",
    "dtheta = (theta_max - theta_min) / (ntheta - 1)\n",
    "theta = np.linspace(theta_min, theta_max, ntheta)\n",
    "\n",
    "dphi = (phi_max - phi_min) / (nphi - 1)\n",
    "phi = np.linspace(phi_min, phi_max, nphi)\n",
    "\n",
    "rtp = np.moveaxis(\n",
    "    np.stack(np.meshgrid(rho, theta, phi, indexing=\"ij\")),\n",
    "    0, \n",
    "    -1\n",
    ")\n",
    "vp_new = vp.resample(rtp.reshape(-1, 3)).reshape(rtp.shape[:-1])\n",
    "vs_new = vs.resample(rtp.reshape(-1, 3)).reshape(rtp.shape[:-1])\n",
    "\n",
    "vp = pykonal.field.ScalarField3D(coord_sys=\"spherical\")\n",
    "vs = pykonal.field.ScalarField3D(coord_sys=\"spherical\")\n",
    "vp.min_coords = vs.min_coords = rho_min, theta_min, phi_min\n",
    "vp.node_intervals = vs.node_intervals = drho, dtheta, dphi\n",
    "vp.npts = vs.npts = nrho, ntheta, nphi\n",
    "vp.values = vp_new\n",
    "vs.values = vs_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute traveltime-lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(args, vp=vp, vs=vs):\n",
    "    station_id, coords = args[0], geo2sph(args[1].values)\n",
    "    print(station_id)\n",
    "    solver = compute_traveltime_lookup_table(coords, vp)\n",
    "    solver.tt.savez(f\"/home/malcolmw/scratch/traveltimes/{station_id}.P.npz\")\n",
    "    solver = compute_traveltime_lookup_table(coords, vs)\n",
    "    solver.tt.savez(f\"/home/malcolmw/scratch/traveltimes/{station_id}.S.npz\")\n",
    "    \n",
    "with mp.Pool() as pool:\n",
    "    pool.map(target, stations_dataframe.iterrows())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locate events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arrivals(arrivals_dataframe):\n",
    "    for event_id in arrivals_dataframe.index.unique()[:]:\n",
    "        yield (event_id, arrivals_dataframe.loc[event_id].set_index([\"station_id\", \"phase\"]).to_dict()[\"time\"])\n",
    "    \n",
    "def target(args, nodes=vp.nodes):\n",
    "    event_id, arrivals = args\n",
    "    print(f\"Locating event #{event_id}\")\n",
    "    tt_calculators = {\n",
    "        key: pykonal.field.load(f\"/home/malcolmw/scratch/traveltimes/{key[0]}.{key[1]}.npz\") for key in arrivals\n",
    "    }\n",
    "    locator = EQLocator(\n",
    "        arrivals=arrivals,\n",
    "        tt_calculators=tt_calculators,\n",
    "        grid=nodes\n",
    "    )\n",
    "    return (event_id, locator.locate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(4) as pool:\n",
    "    locations = pool.map(target,  generate_arrivals(arrivals_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relocated_events_dataframe = pd.DataFrame([[loc[0], *loc[1]] for loc in locations], columns=[\"event_id\", \"lat\", \"lon\", \"depth\", \"time\"]).set_index(\"event_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_events_dataframe = db[\"origin\"][db[\"origin\"][\"event_id\"].isin(arrivals_dataframe.index.unique())].set_index(\"event_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig = plt.figure()\n",
    "ax00 = fig.add_subplot(2, 2, 1)\n",
    "ax01 = fig.add_subplot(2, 2, 2)\n",
    "ax10 = fig.add_subplot(2, 2, 3)\n",
    "ax11 = fig.add_subplot(2, 2, 4)\n",
    "for ax, key in ((ax00, \"lat\"), (ax01, \"lon\"), (ax10, \"depth\"), (ax11, \"time\")):\n",
    "    ax.hist(original_events_dataframe[key] - relocated_events_dataframe[key], bins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax1 = fig.add_subplot(1, 3, 1, aspect=1)\n",
    "ax2 = fig.add_subplot(1, 3, 2, aspect=1)\n",
    "ax3 = fig.add_subplot(1, 3, 3, aspect=1)\n",
    "ax1.scatter(\n",
    "    original_events_dataframe[\"lon\"], \n",
    "    original_events_dataframe[\"lat\"],\n",
    "    c=original_events_dataframe[\"depth\"],\n",
    "    vmin=0,\n",
    "    vmax=25\n",
    ")\n",
    "ax2.scatter(\n",
    "    relocated_events_dataframe[\"lon\"],\n",
    "    relocated_events_dataframe[\"lat\"],\n",
    "    c=original_events_dataframe[\"depth\"],\n",
    "    vmin=0,\n",
    "    vmax=25\n",
    ")\n",
    "ax3.quiver(\n",
    "    original_events_dataframe[\"lon\"], \n",
    "    original_events_dataframe[\"lat\"],\n",
    "    relocated_events_dataframe[\"lon\"] - original_events_dataframe[\"lon\"],\n",
    "    relocated_events_dataframe[\"lat\"] - original_events_dataframe[\"lat\"],\n",
    ")\n",
    "for ax in (ax1, ax2, ax3):\n",
    "    ax.set_xlim(-117, -116)\n",
    "    ax.set_ylim(33, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(1, 1, 1, aspect=1)\n",
    "ax.quiver(\n",
    "    original_events_dataframe[\"lon\"], \n",
    "    original_events_dataframe[\"lat\"],\n",
    "    relocated_events_dataframe[\"lon\"] - original_events_dataframe[\"lon\"],\n",
    "    relocated_events_dataframe[\"lat\"] - original_events_dataframe[\"lat\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_id, origin in relocated_events_dataframe.iterrows():\n",
    "    arrivals = arrivals_dataframe.loc[event_id].set_index([\"station_id\", \"phase\"]).to_dict()[\"time\"]\n",
    "    tt_calculators = {\n",
    "        key: pykonal.field.load(f\"/home/malcolmw/scratch/traveltimes/{key[0]}.{key[1]}.npz\") for key in arrivals\n",
    "    }\n",
    "    locator = EQLocator(\n",
    "        arrivals=arrivals,\n",
    "        tt_calculators=tt_calculators,\n",
    "        grid=vp.nodes\n",
    "    )\n",
    "    print(\n",
    "        event_id, \n",
    "        locator.cost(origin[[\"lat\", \"lon\", \"depth\", \"time\"]].values), \n",
    "        locator.cost(original_events_dataframe.loc[event_id, [\"lat\", \"lon\", \"depth\", \"time\"]].values)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
